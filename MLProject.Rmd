---
title: "MLProject"
author: "noman"
date: "03/01/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Code part

# loading required libraries
```{r}
library(caret)
library(randomForest)
```
# reading the data and identifying NA's
```{r}
trainingData <- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
testData  <- read.csv("pml-testing.csv", na.strings = c("NA", "#DIV/0!", ""))
```
# removing NA from the training and testing data
```{r}
comps <- complete.cases(t(trainingData)) & complete.cases(t(testData))
trainingData <- trainingData[,comps]
testData  <- testData[,comps]
```
# removing unnecessary data
```{r}
trainingData <- trainingData[,-c(1,3,4,5,6,7)]
testData <- testData[,-c(1,3,4,5,6,7)]
```

# splitting data into training and validation sets
```{r}
set.seed(111)
inTrainingData <- createDataPartition(trainingData$classe, p=0.6, list=FALSE)
trainingData2 <- trainingData[inTrainingData,]
validationData <- trainingData[-inTrainingData,]
```
# running a model model on training data set
```{r}
predictionModel <- randomForest(as.factor(classe)~., data=trainingData2)
```
# analysing the model
```{r}
predictOnTraining <- predict(predictionModel, trainingData2)
predAccOnTraining <- sum(predictOnTraining==trainingData2$classe)/length(predictOnTraining)
paste("Accuracy of the model on training set is ",predAccOnTraining*100, " %")
```

# running the model on validation data and checking accuracy of predictions
```{r}
predictOnValidation <- predict(predictionModel, newdata=validationData)
predAccOnValidation <- sum(predictOnValidation==validationData$classe)/length(predictOnValidation)
paste("Accuracy of the model on validation data set is ",predAccOnValidation*100, "%")
```

# testing the model on test set
```{r}
predictOnTesting <- predict(predictionModel, newdata=testData)
predAccOnTesting <- sum(predictOnTesting==testData$classe)/length(predictOnTesting)
print("Predictions based on the model (on testing data):"); 
predictOnTesting
```
## Procedure

Here is the procedure followed above:
- The data files (both training and testing) contain 160 variables. Variables (100) containing NA entries were discarded.
- Additional 6 variables apparently not useful for the prediction were also excluded.
- The training data set is divided into training and validation dataset.
- Prediction model is built upon training dataset. The model understandbly showed 100% accuracy in predicting classe.
- The model is run on validation data set, which show around more than 90% accuracy on this dataset.
- Based on the model, 20 predictions are made on the testing data set.